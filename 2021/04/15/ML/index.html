<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/epic-180x180">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/epic-32x32">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="More">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning">
<meta property="og:url" content="http://example.com/2021/04/15/ML/index.html">
<meta property="og:site_name" content="@Xinjian Pan">
<meta property="og:description" content="More">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_mim_distance.JPG">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_var.JPG">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_1.JPG">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_2.JPG">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/image-20210416140003841.png">
<meta property="og:image" content="https://images0.cnblogs.com/blog/670089/201501/081543500006068.png">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/sigmoid.JPG">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=p_k+=+(%5Cfrac+%7B1%7D%7B2%7D,%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B8%7D,%5Cfrac+%7B1%7D%7B8%7D)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=q_k+=+(%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B4%7D)">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D+*+%5Clog_2+4+++%5Cfrac%7B1%7D%7B4%7D+*+%5Clog_2+4+++%5Cfrac%7B1%7D%7B8%7D+*+%5Clog_2+4+++%5Cfrac%7B1%7D%7B8%7D+*+%5Clog_2+4+=+2">
<meta property="og:image" content="https://habrastorage.org/files/010/ded/77e/010ded77e8d0454b99f0cafd3d962613.png">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-a2a73f43adcbb0bf4b9bae19b9495f81_1440w.png">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=S_%7Bj%7D+">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/27695029-4f327793a9d758b6.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/27695029-6c25b23439b92b7a.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181102194631741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0MjAyODM=,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/b/ba/Normalverteilung.PNG">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Normal_Distribution_CDF.svg/1920px-Normal_Distribution_CDF.svg.png">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Discrete_probability_distrib.svg/1920px-Discrete_probability_distrib.svg.png">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/488px-Poisson_pmf.svg.png">
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Poisson_cdf.svg/1024px-Poisson_cdf.svg.png">
<meta property="og:image" content="https://www.qiujiawei.com/images/2016.8/1.png">
<meta property="og:image" content="https://pic1.zhimg.com/50/v2-5eb5e035330831cd01d8fbea8d5b51e7_720w.jpg?source=1940ef5c">
<meta property="og:image" content="https://pic1.zhimg.com/80/v2-5eb5e035330831cd01d8fbea8d5b51e7_1440w.jpg?source=1940ef5c">
<meta property="og:image" content="https://pica.zhimg.com/50/v2-b082814755a62fcf676f3d08c70d2d0d_720w.jpg?source=1940ef5c">
<meta property="og:image" content="https://pica.zhimg.com/80/v2-b082814755a62fcf676f3d08c70d2d0d_1440w.jpg?source=1940ef5c">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5CSigma+P(x,+y)+=+1">
<meta property="og:image" content="https://pic2.zhimg.com/50/v2-11f8a39d629e2649146bf12794fe310c_720w.jpg?source=1940ef5c">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-11f8a39d629e2649146bf12794fe310c_1440w.jpg?source=1940ef5c">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Csum_%7By%7D%7BP(y+%7C+x)%7D+=+1+">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?f%5Cleft&space;(&space;x&space;%5Cright&space;)=f%5Cleft&space;(&space;x_k&space;%5Cright&space;)+%7Bf%7D%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%5Cleft&space;(&space;x-x_k&space;%5Cright&space;)+%5Cfrac%7B1%7D%7B2%7D%7Bf%7D%27%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%5Cleft&space;(&space;x-x_k&space;%5Cright&space;)%5E2">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%7Bf%7D%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)+%7Bf%7D%27%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%5Cleft&space;(&space;x-x_k&space;%5Cright&space;)=0">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?x=x_k-%5Cfrac%7B%7Bf%7D%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%7D%7B%7Bf%7D%27%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%7D">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/model.JPG">
<meta property="og:image" content="https://pica.zhimg.com/v2-a026e24156e13a1d14c43df26b9bd2a4_r.jpg?source=1940ef5c">
<meta property="og:image" content="https://pica.zhimg.com/v2-f6edae58134c5a26687c3883af48d5d5_r.jpg?source=1940ef5c">
<meta property="og:image" content="https://pic3.zhimg.com/v2-3aaa69f70754c469bca5c8e4c3e161db_r.jpg?source=1940ef5c">
<meta property="og:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/MSE.JPG">
<meta property="og:image" content="https://img-blog.csdnimg.cn/img_convert/9c16e9482a860b7f354bcd8a4d5c418e.png">
<meta property="og:image" content="https://img-blog.csdn.net/20160611221836609?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20160611221842349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20160611221847422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20160611221851021?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20160611222004158?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://img-blog.csdn.net/20160611211339152?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7DGini(D)&=%5Csum_%7Bi=1%7D%5E%7Bn%7D%7Bp(x_i)*(1-p(x_i))%7D+%5C%5C&=1-%5Csum_%7Bi=1%7D%5E%7Bn%7D%7B%7Bp(x_i)%7D%5E2%7D%5Cend%7Baligned%7D++%5C%5C">
<meta property="og:image" content="https://wikimedia.org/api/rest_v1/media/math/render/svg/2fe7849758309b19fa49cad2f5e214fb4fbb8044">
<meta property="og:image" content="https://wikimedia.org/api/rest_v1/media/math/render/svg/e76702c87ce1c681ed1da8213125963524ca0ee6">
<meta property="og:image" content="https://wikimedia.org/api/rest_v1/media/math/render/svg/5decdfc7edb14bc3f4dd541d39559ea9a3088f61">
<meta property="og:image" content="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne_1.JPG?raw=true">
<meta property="og:image" content="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne2.JPG?raw=true">
<meta property="og:image" content="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne3.JPG?raw=true">
<meta property="og:image" content="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne4.JPG?raw=true">
<meta property="og:image" content="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne6.JPG?raw=true">
<meta property="og:image" content="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne5.JPG?raw=true">
<meta property="article:published_time" content="2021-04-15T18:07:25.834Z">
<meta property="article:modified_time" content="2022-05-02T18:21:27.346Z">
<meta property="article:author" content="Xinjian Pan  👨🏻‍💻">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_mim_distance.JPG">

<link rel="canonical" href="http://example.com/2021/04/15/ML/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Machine Learning | @Xinjian Pan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">@Xinjian Pan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about-me">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About Me</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">16</span></a>

  </li>
        <li class="menu-item menu-item-cv">

    <a href="/CV/" rel="section"><i class="fa fa-archive fa-fw"></i>CV</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/04/15/ML/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Xinjian Pan  👨🏻‍💻">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="@Xinjian Pan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Machine Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-04-15 20:07:25" itemprop="dateCreated datePublished" datetime="2021-04-15T20:07:25+02:00">2021-04-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-05-02 20:21:27" itemprop="dateModified" datetime="2022-05-02T20:21:27+02:00">2022-05-02</time>
              </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
            <div class="post-description">More</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <span id="more"></span>

<!-- toc -->

<ul>
<li><a href="#%E5%90%91%E9%87%8F%E5%92%8C%E7%9F%A9%E9%98%B5">向量和矩阵</a></li>
<li><a href="#%E5%90%91%E9%87%8F%E7%9A%84%E8%8C%83%E6%95%B0norm">向量的范数(norm)</a></li>
<li><a href="#%E9%80%86%E7%9F%A9%E9%98%B5matrix-inversion">逆矩阵(matrix inversion)</a></li>
<li><a href="#%E5%8D%95%E4%BD%8D%E5%90%91%E9%87%8F">单位向量</a></li>
<li><a href="#%E6%AD%A3%E4%BA%A4%E7%9F%A9%E9%98%B5orthogonal-matrix">正交矩阵（orthogonal matrix）</a></li>
<li><a href="#%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3eigendecomposition">特征分解（eigendecomposition)</a></li>
<li><a href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3singular-value-decomposition-svd">奇异值分解（singular value decomposition, SVD)</a></li>
<li><a href="#%E8%BF%B9%E8%BF%90%E7%AE%97">迹运算</a></li>
<li><a href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90principal-components-analysis-pca">主成分分析（principal components analysis, PCA）</a></li>
<li><a href="#%E6%A6%82%E7%8E%87%E8%AE%BA">概率论</a><ul>
<li><a href="#%E4%BF%A1%E4%BB%BB%E5%BA%A6degree-of-belief">信任度(degree of belief):</a></li>
<li><a href="#%E5%BD%92%E4%B8%80%E5%8C%96normalized">归一化（normalized）</a></li>
<li><a href="#%E7%A6%BB%E6%95%A3%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83uniform-distribution">离散均匀分布（uniform distribution）:</a></li>
<li><a href="#%E6%A6%82%E7%8E%87%E5%AF%86%E5%BA%A6%E5%87%BD%E6%95%B0probability-density-function">概率密度函数（probability density function):</a><ul>
<li><a href="#%E8%BF%9E%E7%BB%AD%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83">连续均匀分布:</a></li>
</ul>
</li>
<li><a href="#%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99chain-rule">链式法则（chain rule）:</a></li>
<li><a href="#%E7%8B%AC%E7%AB%8B%E6%80%A7independent">独立性（independent）:</a></li>
<li><a href="#%E6%9C%9F%E6%9C%9Bexpectation">期望（expectation）:</a></li>
<li><a href="#%E6%96%B9%E5%B7%AEvariance">方差（variance）:</a></li>
<li><a href="#%E5%8D%8F%E6%96%B9%E5%B7%AEcovariance">协方差（covariance）:</a></li>
<li><a href="#%E5%B8%B8%E7%94%A8%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83">常用概率分布</a></li>
</ul>
</li>
<li><a href="#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97">数值计算</a><ul>
<li><a href="#jacobian%E7%9F%A9%E9%98%B5">Jacobian矩阵</a></li>
</ul>
</li>
<li><a href="#hessian-%E7%9F%A9%E9%98%B5">Hessian 矩阵</a><ul>
<li><a href="#karushkuhntuckerkkt">Karush–Kuhn–Tucker（KKT）</a></li>
</ul>
</li>
<li><a href="#%E8%B7%9D%E7%A6%BB%E9%80%89%E6%8B%A9%E5%85%AC%E5%BC%8F">距离选择公式</a><ul>
<li><a href="#%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E8%B7%9D%E7%A6%BB">欧几里得距离</a></li>
<li><a href="#%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB">曼哈顿距离</a></li>
</ul>
</li>
<li><a href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%9A%E7%90%86">贝叶斯定理</a></li>
<li><a href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AFnaive-bayesian-algorithm">朴素贝叶斯(Naive Bayesian algorithm)</a><ul>
<li><a href="#%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91%E5%A4%84%E7%90%86laplace-smoothing">拉普拉斯平滑处理(Laplace Smoothing)</a></li>
</ul>
</li>
<li><a href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0loss-function%E6%88%96%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0cost-function">损失函数(loss function)或代价函数(cost function)</a><ul>
<li><a href="#%E5%B9%B3%E6%96%B9%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0quadratic-loss-function">平方损失函数(quadratic loss function)</a></li>
<li><a href="#%E7%BB%8F%E9%AA%8C%E6%8D%9F%E5%A4%B1empirical-loss-r_emp">经验损失(empirical loss) $R_{emp}$</a></li>
<li><a href="#regression-loss">Regression Loss</a><ul>
<li><a href="#l_1-loss-laplace-loss-absolute-loss">$l_1$ Loss &#x2F; Laplace Loss &#x2F; Absolute Loss</a></li>
<li><a href="#l_2-loss-square-loss">$l_2$ Loss &#x2F; Square Loss</a></li>
<li><a href="#huber-loss">Huber Loss</a></li>
</ul>
</li>
<li><a href="#classification-loss">Classification Loss</a><ul>
<li><a href="#margin">Margin</a></li>
<li><a href="#zero-one">Zero-one</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#gramm%E7%9F%A9%E9%98%B5">Gramm矩阵</a></li>
<li><a href="#mercers-theorem">Mercer’s Theorem</a></li>
<li><a href="#kernel-trick">Kernel Trick</a></li>
<li><a href="#%E6%9D%B0%E6%A3%AE%E4%B8%8D%E7%AD%89%E5%BC%8Fjensens-inequality">杰森不等式(Jensen’s inequality)</a></li>
<li><a href="#%E5%87%B8%E5%87%BD%E6%95%B0">凸函数</a></li>
<li><a href="#weak-max-min-inequality">Weak Max-Min Inequality</a></li>
<li><a href="#%E6%9D%BE%E5%BC%9B%E4%BA%92%E8%A1%A5complementary-slackness">松弛互补（Complementary slackness）</a></li>
<li><a href="#%E4%B8%A4%E5%B9%B3%E8%A1%8C%E7%BA%BF%E8%B7%9D%E7%A6%BB%E5%85%AC%E5%BC%8F">两平行线距离公式</a></li>
<li><a href="#pdfprobability-density-function">PDF(Probability density function)</a></li>
<li><a href="#cdfcumulative-distribution-function">CDF(Cumulative distribution function)</a></li>
<li><a href="#pmfprobability-mass-function">PMF(probability mass function)</a></li>
<li><a href="#%E6%B3%8A%E6%9D%BE%E5%88%86%E5%B8%83poisson-distribution">泊松分布(Poisson distribution)</a></li>
<li><a href="#variance">Variance</a></li>
<li><a href="#%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83%E5%92%8C%E4%BA%8C%E9%A1%B9%E5%88%86%E5%B8%83">伯努利分布和二项分布</a></li>
<li><a href="#z%E5%88%86%E5%B8%83%E5%92%8Ct%E5%88%86%E5%B8%83">Z分布和T分布</a></li>
<li><a href="#%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E7%A7%AF%E5%88%86">蒙特卡洛积分</a></li>
<li><a href="#%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B%E5%92%8C%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B">判别模型和生成模型</a></li>
<li><a href="#%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1mle">极大似然估计(MLE)</a></li>
<li><a href="#%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E6%A6%82%E7%8E%87-map">最大后验概率 (MAP)</a></li>
<li><a href="#%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80">泰勒展开</a></li>
<li><a href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95">梯度下降法</a></li>
<li><a href="#%E7%89%9B%E9%A1%BF%E6%B3%95">牛顿法</a></li>
</ul>
<ul>
<li><a href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80">机器学习基础</a><ul>
<li><a href="#%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95unsupervised-learning-algorithm">无监督学习算法（unsupervised learning algorithm）</a></li>
<li><a href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95supervised-learning-algorithm">监督学习算法（supervised learning algorithm)</a></li>
<li><a href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0rein-forcement-learning%E7%AE%97%E6%B3%95">强化学习（rein-forcement learning）算法</a></li>
<li><a href="#%E8%AE%BE%E8%AE%A1%E7%9F%A9%E9%98%B5design-matrix">设计矩阵（design matrix）</a></li>
<li><a href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AEtraining-error">训练误差（training error）</a></li>
<li><a href="#%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AEtest-error">测试误差（test error）</a></li>
<li><a href="#%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80-occams-razor">奥卡姆剃刀 Occam’s Razor</a></li>
<li><a href="#vapnik-chervonenkis-%E7%BB%B4%E5%BA%A6">Vapnik-Chervonenkis 维度</a></li>
<li><a href="#%E8%B4%9D%E5%8F%B6%E6%96%AF%E8%AF%AF%E5%B7%AEbayes-error">贝叶斯误差（Bayes error）</a></li>
<li><a href="#%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86no-freelunch-theorem">没有免费午餐定理（no freelunch theorem）</a></li>
<li><a href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8Fweight-decay">权重衰减（weight decay）</a></li>
<li><a href="#%E6%AD%A3%E5%88%99%E5%8C%96regularization">正则化（regularization）</a><ul>
<li><a href="#lasso-regularization">Lasso Regularization</a></li>
<li><a href="#ridge-regularization">Ridge Regularization</a></li>
<li><a href="#%E4%B8%BA%E4%BB%80%E4%B9%88lasso-%E6%AF%94ridge-%E6%9B%B4%E7%A8%80%E7%96%8F-ridge%E6%AF%94lasso%E6%9B%B4%E5%B9%B3%E6%BB%91">为什么Lasso 比Ridge 更稀疏， Ridge比Lasso更平滑</a></li>
</ul>
</li>
<li><a href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81">交叉验证</a></li>
<li><a href="#%E9%A2%84%E6%B5%8B%E8%AF%AF%E5%B7%AEmeasures-prediction-errormse">预测误差（measures prediction error,MSE）</a></li>
<li><a href="#underfitting-overfitting">Underfitting, Overfitting</a></li>
<li><a href="#%E6%84%9F%E7%9F%A5%E6%9C%BAperceptron">感知机(Perceptron)</a></li>
<li><a href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">监督学习</a><ul>
<li><a href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92logistic-regression">逻辑回归（logistic regression）</a></li>
<li><a href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BAsupport-vector-machine-svm">支持向量机（support vector machine, SVM)</a></li>
<li><a href="#%E5%86%B3%E7%AD%96%E6%A0%91">决策树</a></li>
<li><a href="#knn">KNN</a></li>
</ul>
</li>
<li><a href="#k-mean-%E7%AE%97%E6%B3%95">K Mean 算法</a></li>
<li><a href="#em%E7%AE%97%E6%B3%95">EM算法</a></li>
<li><a href="#bootstrap-%E9%87%87%E6%A0%B7%E6%B3%95">bootstrap 采样法</a></li>
<li><a href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">线性回归</a></li>
<li><a href="#%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">随机森林</a></li>
<li><a href="#boosting-and-bagging">Boosting and bagging</a></li>
<li><a href="#xgboost">XGBoost</a></li>
<li><a href="#local-linear-embedding-lle">Local Linear Embedding (LLE)</a></li>
<li><a href="#sne-and-t-sne">SNE and t-SNE</a><ul>
<li><a href="#sne">SNE</a></li>
<li><a href="#t-sne">t-SNE</a></li>
</ul>
</li>
<li><a href="#cnn">CNN</a></li>
</ul>
</li>
</ul>
<!-- tocstop -->

<p>[TOC]</p>
<h2><span id="向量和矩阵">向量和矩阵</span></h2><p><strong>标量（scalar)</strong>: 一个标量表示一个单独的数</p>
<p><strong>向量(vector)</strong>: 一个向量表示一组有序排列的数。</p>
<p><strong>矩阵（matrix)</strong>: 矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。</p>
<p><strong>张量（tensor)</strong>: 一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。</p>
<p><strong>矩阵乘法后矩阵的大小</strong>: $$ a_{ik}*b_{kj}&#x3D;c_{ij}$$ </p>
<h2><span id="向量的范数norm">向量的范数(norm)</span></h2><p>定义一个向量为：$\vec{a}&#x3D;[-5, 6, 8, -10]$。任意一组向量设为$\vec{x}&#x3D;(x_1,x_2,…,x_N)$。其不同范数求解如下：</p>
<ul>
<li>向量的1范数：向量的各个元素的绝对值之和，上述向量$\vec{a}$的1范数结果就是：29。</li>
</ul>
<p>$$<br>\Vert\vec{x}\Vert_1&#x3D;\sum_{i&#x3D;1}^N\vert{x_i}\vert<br>$$</p>
<ul>
<li>向量的2范数：向量的每个元素的平方和再开平方根，上述$\vec{a}$的2范数结果就是：15。</li>
</ul>
<p>$$<br>\Vert\vec{x}\Vert_2&#x3D;\sqrt{\sum_{i&#x3D;1}^N{\vert{x_i}\vert}^2}<br>$$</p>
<ul>
<li>向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量$\vec{a}$的负无穷范数结果就是：5。</li>
</ul>
<p>$$<br>\Vert\vec{x}\Vert_{-\infty}&#x3D;\min{|{x_i}|}<br>$$</p>
<ul>
<li>向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量$\vec{a}$的正无穷范数结果就是：10。</li>
</ul>
<p>$$<br>\Vert\vec{x}\Vert_{+\infty}&#x3D;\max{|{x_i}|}<br>$$</p>
<ul>
<li>向量的p范数：</li>
</ul>
<p>$$<br>L_p&#x3D;\Vert\vec{x}\Vert_p&#x3D;\sqrt[p]{\sum_{i&#x3D;1}^{N}|{x_i}|^p}<br>$$</p>
<ul>
<li>两个向量的点积（dot product）可以用范数来表示:</li>
</ul>
<p>$$<br>x^Ty&#x3D;\lVert x\rVert_2\lVert y\rVert_2cos\theta<br>$$</p>
<h2><span id="逆矩阵matrix-inversion">逆矩阵(matrix inversion)</span></h2><p>$$<br>A^{-1}A&#x3D;I<br>$$</p>
<p>or:<br>$$<br>AA^{-1}&#x3D;I<br>$$</p>
<h2><span id="单位向量">单位向量</span></h2><p>单位向量（unit vector）是具有单位范数（unit norm）的向量：<br>$$<br>\lVert x \rVert_2&#x3D;1<br>$$</p>
<h2><span id="正交矩阵orthogonal-matrix">正交矩阵（orthogonal matrix）</span></h2><p>是指行向量和列向量是分别标准正交的方阵：<br>$$<br>A^TA&#x3D;AA^T&#x3D;1<br>$$<br>即:<br>$$<br>A^{-1}&#x3D;A^T<br>$$</p>
<h2><span id="特征分解eigendecomposition">特征分解（eigendecomposition)</span></h2><p>方阵A 的特征向量（eigenvector）是指与A 相乘后相当于对该向量进行缩放<br>的非零向量v：<br>$$<br>Av&#x3D;\lambda v<br>$$</p>
<blockquote>
<p>v为右特征向量，$\lambda$为特征值</p>
</blockquote>
<p>所有特征值都是正数的矩阵被称为正定（positive definite)；所有特征值都是非负数的矩阵被称为半正定（positive semidefinite）。同样地，所有特征值都是负数的矩阵被称为负定（negative definite)；所有特征值都是非正数的矩阵被称为半负定（negative semidefinite）</p>
<p>半正定矩阵:<br>$$<br>\forall x, x^TAx \ge 0<br>$$<br>正定矩阵:<br>$$<br>x^TAx&#x3D;0, \Rightarrow x&#x3D;0<br>$$</p>
<h2><span id="奇异值分解singular-value-decomposition-svd">奇异值分解（singular value decomposition, SVD)</span></h2><p>将矩阵A 分解成三个矩阵的乘积:<br>$$<br>A&#x3D;UDV^T<br>$$</p>
<blockquote>
<p>A 是一个mxn 的矩阵，那么U 是一个mxm 的矩阵，D 是一个mxn 的矩阵，V 是一个nxn 矩阵</p>
<p>矩阵U 和V 都定义为正交矩阵，而矩阵D 定义为对角矩阵。注意，矩阵D 不一定是方阵。</p>
<p>A 的左奇异向量（left singular vector）是$AA^⊤$ 的特征向量。A 的右奇异向量（right singularvector）是$A^⊤A$ 的特征向量。A 的非零奇异值是$A^⊤A$ 特征值的平方根，同时也是A$A^⊤$ 特征值的平方根。</p>
</blockquote>
<h2><span id="迹运算">迹运算</span></h2><p>迹运算返回的是矩阵对角元素的和：<br>$$<br>Tr(A) &#x3D; \sum_iA_{i,i}<br>$$</p>
<p>$$<br>Tr(ABC)&#x3D;Tr(CAB)&#x3D;Tr(BCA)<br>$$</p>
<h2><span id="主成分分析principal-components-analysis-pca">主成分分析（principal components analysis, PCA）</span></h2><p><strong>PCA主要要求:</strong></p>
<ol>
<li><strong>最大方差</strong></li>
<li><strong>最小错误</strong></li>
</ol>
<blockquote>
<p>在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。</p>
</blockquote>
<p><strong>最小错误</strong>:</p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_mim_distance.JPG" alt="avatar"></p>
<p><strong>最大方差</strong>:</p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_var.JPG" alt="avatar"></p>
<blockquote>
<p>Var(X)&#x3D;E($X^2$)-$E(X)^2$ -&gt; centered: E(X)&#x3D;0</p>
</blockquote>
<p><strong>用拉格朗日乘子法</strong>:<br>$$<br>L(w,\lambda)&#x3D;w^TSw+\lambda(1-w^Tw)\<br>\frac{\delta L}{\delta w} &#x3D; 2Sw-2\lambda w\<br>Sw&#x3D;\lambda w\<br>(S-\lambda I)w&#x3D;0<br>$$<br><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_1.JPG" alt="avatar"></p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/pca_2.JPG" alt="avatar"></p>
<p><strong>Result</strong>:</p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/image-20210416140003841.png"></p>
<p><strong>4. PCA理论意义</strong>:</p>
<p>PCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA要保证降维后，还要保证数据的特性损失最小。</p>
<blockquote>
<p>PCA对于outliers并不是robust</p>
</blockquote>
<p>可以配合Whitening，</p>
<p><img src="https://images0.cnblogs.com/blog/670089/201501/081543500006068.png" alt="img"></p>
<p>使各自的variance都为1</p>
<h2><span id="概率论">概率论</span></h2><h3><span id="信任度degree-of-belief">信任度(degree of belief):</span></h3><p>在医生诊断病人的例子中，其中1 表示非常肯定病人患有流感，而0 表示非常肯定病人没有流感。</p>
<h3><span id="归一化normalized">归一化（normalized）</span></h3><p>$$<br>\sum P(x)&#x3D;1<br>$$</p>
<h3><span id="离散均匀分布uniform-distribution">离散均匀分布（uniform distribution）:</span></h3><p>考虑一个离散型随机变量x 有k 个不同的状态:<br>$$<br>P(x&#x3D;x_i)&#x3D;\frac{1}{k}<br>$$</p>
<h3><span id="概率密度函数probability-density-function">概率密度函数（probability density function):</span></h3><p>研究的对象是连续型随机变量, 且满足以下条件:</p>
<ul>
<li>p 的定义域必须是x 所有可能状态的集合。</li>
<li>$\forall x,p(x)\ge0$</li>
<li>$\int p(x)dx&#x3D;1$</li>
</ul>
<h4><span id="连续均匀分布">连续均匀分布:</span></h4><p>x 在[a; b] 上是均匀分布的:<br>$$<br>u(x;a,b) &#x3D; \frac{1}{b-a}<br>$$</p>
<h3><span id="链式法则chain-rule">链式法则（chain rule）:</span></h3><p>$$<br>P(a, b, c) &#x3D; P(a | b, c)P(b | c)P(c)<br>$$</p>
<h3><span id="独立性independent">独立性（independent）:</span></h3><p>$$<br>\forall x,y\ \ \ p(x,y)&#x3D;p(x)p(y)<br>$$</p>
<h3><span id="期望expectation">期望（expectation）:</span></h3><p><strong>离散期望</strong>:<br>$$<br>\sum P(x)f(x)<br>$$<br><strong>连续期望:</strong><br>$$<br>\int p(x)f(X)dx<br>$$</p>
<h3><span id="方差variance">方差（variance）:</span></h3><p>$$<br>Var(x)&#x3D;E[x^2-2xE[x]+E[x]^2]&#x3D;E[x^2]-2E[x]E[x]-E[x]^2&#x3D;E[x^2]-E[x]^2<br>$$</p>
<h3><span id="协方差covariance">协方差（covariance）:</span></h3><p>$$<br>Var(x)&#x3D;cov(x,x)&#x3D;E[(X-E[X])(Y-E[Y])]&#x3D;E[XY]-E[X]E[Y]<br>$$</p>
<p>协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很远。如果协方差是正的，那么两个变量都倾向于同时取得相对较大的值。如果协方差是负的，那么其中一个变量倾向于取得相对较大的值的同时，另一个变量倾向于取得相对较小的值，反之亦然。其他的衡量指标如相关系数（correlation）将每个变量的贡献归一化，为了只衡量变量的相关性而不受各个变量尺度大小的影响。</p>
<h3><span id="常用概率分布">常用概率分布</span></h3><p><strong>Bernoulli 分布:</strong></p>
<p>Bernoulli 分布（Bernoulli distribution）是单个二值随机变量的分布:<br>$$<br>P(x&#x3D;1)&#x3D;\phi\<br>P(x&#x3D;0)&#x3D;1-\phi\<br>P(X&#x3D;x)&#x3D;\phi^x(1-\phi)^{1-x}<br>$$</p>
<p><strong>高斯分布(Gaussian distribution)</strong></p>
<p>实数上最常用的分布就是正态分布（normal distribution），也称为高斯分布（Gaussian distribution）：<br>$$<br>N(x;\mu,\sigma^2)&#x3D;\sqrt{\frac{1}{2\pi\sigma^2}}exp(-\frac{1}{2\sigma^2}(x-\mu)^2)<br>$$</p>
<p><strong>多维正态分布（multivariatenormal distribution）:</strong><br>$$<br>N(x;\mu,\Sigma)&#x3D;\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))<br>$$</p>
<p><strong>Beta 分布:</strong></p>
<p>$\alpha &#x3D; 1, \beta &#x3D;1$ -&gt; 没有经验</p>
<p>$\alpha&#x3D;5,\beta&#x3D;5$ -&gt; 默认是均匀分布</p>
<p>Beta分布通常用于二项分布的先验概率</p>
<p><strong>Dirac 分布（Dirac delta function）:</strong><br>$$<br>p(x)&#x3D;\delta(x-u)<br>$$<br>在u处无穷大，其他处为0</p>
<p>Dirac 分布经常作为经验分布（empirical distribution）的一个组成部分出现：<br>$$<br>p(x)&#x3D;\frac{1}{m}\sum \delta(x-x^{i})<br>$$<br>经验分布将概率密度$\frac{1}{m}$ 赋给m 个点$x^{(1)},…,x^{(m)}$ 中的每一个，这些点是给定的<br>数据集或者采样的集合。通常多项分布的先验概率</p>
<p><strong>logistic sigmoid 函数’:</strong><br>$$<br>\sigma(x)&#x3D;\frac{1}{1+exp(-x)}\<br>\Rightarrow\sigma(x)&#x3D;\frac{exp(x)}{exp(x)+exp(0)}\<br>$$</p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/sigmoid.JPG"></p>
<p>多用于多标签分类，将任意的值转换为[0,1]之间，多用于输出层，可能有多个正确答案，可以选择多个答案<br>$$<br>sigmoid导数：sigmoid*(1-sigmoid)<br>$$<br>缺点：</p>
<ul>
<li>两端梯度消失</li>
<li>为指数形式，计算量大</li>
</ul>
<p><strong>softmax函数</strong></p>
<p><strong>Softmax函数</strong>，又称归一化指数函数，输出为互斥输出<br>$$<br>softmax(x) &#x3D; \frac{e^{x_i}}{\sum{e^{x_i}}}<br>$$<br>输出值的和为1，作为我们的预测目标，softmax作为MLP(多层感知机)的最后一层，并配合以交叉熵损失函数</p>
<p><strong>贝叶斯规则（Bayes’ rule）</strong><br>$$<br>P(x|y)&#x3D;\frac{P(x)P(y|x)}{P(y)}<br>$$</p>
<p><strong>自信息（self-information）:</strong><br>$$<br>I(x)&#x3D;-logP(x)<br>$$<br>不可能发生的事件具有更高的信息量</p>
<p><strong>香农熵（Shannon entropy）:</strong></p>
<p>熵是不确定性的一种度量。可以表示一个事件的自信息量，也就是A包含多少信息。<br>$$<br>H(x)&#x3D;E[I(x)]&#x3D;-E[logP(x)]&#x3D;-pIn(p)-(1-p)In(1-p)<br>$$</p>
<p><strong>KL 散度（Kullback-Leibler (KL) divergence）:</strong><br>$$<br>D_{KL}(P||Q)&#x3D;E[log\frac{P(x)}{Q(x)}]&#x3D;E[logP(x)-logQ(x)]<br>$$<br>如果我们对于同一个随机变量x 有两个单独的概率分布P(x) 和Q(x)，我们可以使用KL 散度（Kullback-Leibler (KL) divergence）来<strong>衡量这两个分布的差异</strong>,KL 散度有很多有用的性质，最重要的是它是<strong>非负的</strong>。<strong>KL 散度为0 当且仅当P 和Q 在离散型变量的情况下是相同的分布</strong>，或者在连续型变量的情况下是‘‘几乎处处’’ 相同的。因为KL 散度是非负的并且衡量的是两个分布之间的差异，它经常被用作分布之间的某种距离。然而，它并不是真的距离因为它不是对称的：对于某些P 和Q，$D_{KL}(P||Q)\ne D_{KL}(Q||P)$ ,</p>
<p><strong>交叉熵（cross-entropy）:</strong><br>$$<br>H(P,Q) &#x3D; H(P) +D_{KL}(P||Q)<br>$$</p>
<p>$$<br>\Rightarrow\ \ H(P,Q)&#x3D;-E[logQ(x)]<br>$$</p>
<p><strong>如果熵是一个常量，那么KL散度和交叉熵在特定条件下等价。</strong>交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。</p>
<p>真实分布 <img src="https://www.zhihu.com/equation?tex=p_k+=+(%5Cfrac+%7B1%7D%7B2%7D,%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B8%7D,%5Cfrac+%7B1%7D%7B8%7D)" alt="[公式]"> ， 非真实分布 <img src="https://www.zhihu.com/equation?tex=q_k+=+(%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B4%7D,%5Cfrac+%7B1%7D%7B4%7D)" alt="[公式]"> ，交叉熵为 <img src="https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D+*+%5Clog_2+4+++%5Cfrac%7B1%7D%7B4%7D+*+%5Clog_2+4+++%5Cfrac%7B1%7D%7B8%7D+*+%5Clog_2+4+++%5Cfrac%7B1%7D%7B8%7D+*+%5Clog_2+4+=+2" alt="[公式]"> </p>
<p><strong>所以逻辑思路是，为了让学到的模型分布更贴近真实数据分布，我们最小化 模型数据分布 与 训练数据之间的KL散度，而因为训练数据的分布是固定的，因此最小化KL散度等价于最小化交叉熵。</strong></p>
<p><strong>因为等价，而且交叉熵更简单更好计算，当然用它</strong></p>
<p><strong>FP, FN, TP, TN</strong></p>
<p>TP: True Positive, 真的预测也为真</p>
<p>TN: True Negative,假的预测也为假</p>
<p>FP: False Positive,假的预测为真</p>
<p>FN: False Nagative,真的预测为假</p>
<p><strong>Precision, Recall and Accurancy</strong></p>
<p>Precision: is the accuracy of the positive predictions<br>$$<br>\frac{TP}{TP+FP}<br>$$<br>Recall: is the accuracy of the positive class<br>$$<br>\frac{TP}{TP+FN}<br>$$<br>Accurancy:<br>$$<br>\frac{TP+NP}{TP+NP+TN+FN}<br>$$<br><strong>True positive rate, False positive rate</strong><br>$$<br>True\ positive\ rate &#x3D; \frac{TP}{TP+FN}<br>$$</p>
<p>$$<br>False\ positive\ rate&#x3D;\frac{FP}{TN+FP}<br>$$</p>
<p><strong>$F_1$ Score</strong></p>
<p>为了平衡Precision和Recall,来给出模型的评估值， $F_1$ Score 就被使用了，$F_1$ Score其实就是Precision和Recall的调和平均数(Harmonic mean)<br>$$<br>2\frac{1}{\frac{1}{Precision}+\frac{1}{Recall}} &#x3D; \frac{2<em>Precision</em>Recall}{Precision+Recall}<br>$$<br><strong>$F_\beta$ Score</strong><br>$$<br>(1+\beta^2)\frac{Precision*Recall}{\beta^2Precision+Recall}<br>$$<br> <strong>PR Curve</strong></p>
<p><img src="https://habrastorage.org/files/010/ded/77e/010ded77e8d0454b99f0cafd3d962613.png"></p>
<p>阈值从大到小，如果一开始Precision为1，则说明高阈值下的分类都为正确，最后recall为1，说明正确分类了，但是错误被分为正确的列子也增加了。</p>
<p>在一定程度上，PR 是矛盾的，所以可以用$F_1 Score$来评定</p>
<p><strong>Receiver Operating Characteristic (ROC) Curve</strong></p>
<p>ROC &#x3D; TPR VS FPR</p>
<p>AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png"></p>
<p>**AUC ROC **</p>
<p>AUC ROC &#x3D; area under the ROC curve</p>
<p>当正负样本的分布发生变化时，ROC曲线的形状能够基本保持不变，而P-R曲线的形状一般会发生较剧烈的变化。</p>
<h2><span id="数值计算">数值计算</span></h2><h3><span id="jacobian矩阵">Jacobian矩阵</span></h3><p>$$<br>J_{i,j} &#x3D; \frac{\delta}{\delta x_j}f(x)_i<br>$$</p>
<h2><span id="hessian-矩阵">Hessian 矩阵</span></h2><p>$$<br>H(f)(x)_{i,j} &#x3D; \frac{\delta ^2}{\delta x_i \delta x_j}f(x)<br>$$</p>
<p>Hessian 矩阵可以看为 Jacobian矩阵的梯度。</p>
<p>当Hessian 是正定的（所有特征值都是正的），则该临界点是局部极小点。因为方向二阶导数在任意方向都是正的，参考单变量的二阶导数测试就能得出此结论。同样的，当Hessian 是负定的（所有特征值都是负的），这个点就是局部极大点。</p>
<p>黑塞矩阵是是对称矩阵，可分解为<br>$$<br>H&#x3D;E\Lambda E^T<br>$$<br>$\Lambda$ 是特征向量，E是单位向量<br>$$<br>Av&#x3D;\lambda v<br>$$<br>$v$为特征向量，$\lambda$ 为特征值，如果特诊值为正，则表明经过线性变化后，方向不变，为负，则方向相反，为0则缩回原点。$\lambda$ 的数值大小表示缩放大小。实对称矩阵，不同特征值对应的特征向量必定正交。</p>
<h3><span id="karushkuhntuckerkkt">Karush–Kuhn–Tucker（KKT）</span></h3><p>加入 g(i) 称为等式约束（equality constraint）。涉及h(j) 的不等式称为不等式约束（inequality constraint）。我们为每个约束引入新的变量$\lambda_i$ 和$\alpha_j$，这些新变量被称为KKT 乘子。<br>$$<br>L(x,\lambda,\alpha) &#x3D; f(x) + \sum \lambda g(x)+\sum \alpha h(x)<br>$$<br>满足条件:</p>
<ul>
<li>$\frac{\delta L}{\delta x}&#x3D;0$</li>
<li>$\lambda \ge 0$</li>
<li>$\lambda h(x)&#x3D;0$</li>
</ul>
<h2><span id="距离选择公式">距离选择公式</span></h2><h3><span id="欧几里得距离">欧几里得距离</span></h3><p>$$<br>d &#x3D; \sqrt{(x_1-x_2)^2+(y_1-y_2)^2}<br>$$</p>
<h3><span id="曼哈顿距离">曼哈顿距离</span></h3><p>$$<br>d &#x3D; |x_1-x_2|+|y_1-y_2|<br>$$</p>
<h2><span id="贝叶斯定理">贝叶斯定理</span></h2><p>$$<br>P(\theta | D) &#x3D; \frac{P(\theta)P(D|\theta)}{P(D)}<br>$$</p>
<blockquote>
<p>$\theta$ 表示模型参数，D表示数据</p>
<p>P($\theta$) 是先验概率，P(D|$\theta$) 为似然函数，P($\theta$|D)是后验概率 </p>
</blockquote>
<h2><span id="朴素贝叶斯naive-bayesian-algorithm">朴素贝叶斯(Naive Bayesian algorithm)</span></h2><p><img src="https://pic2.zhimg.com/80/v2-a2a73f43adcbb0bf4b9bae19b9495f81_1440w.png" alt="avatar"></p>
<p><strong>p(不帅、性格不好、身高矮、不上进|嫁) &#x3D; p(不帅|嫁)*p(性格不好|嫁)*p(身高矮|嫁)*p(不上进|嫁)</strong></p>
<h3><span id="拉普拉斯平滑处理laplace-smoothing">拉普拉斯平滑处理(Laplace Smoothing)</span></h3><p>主要用于解决某些特征未出现在数据集中，从而导致P(特征|类别)为0而导致计算出现问题。</p>
<p>性格特征的个数为爆好，好，不好，三种情况，那么<img src="https://www.zhihu.com/equation?tex=S_%7Bj%7D+" alt="[公式]">为3，则最终概率为1&#x2F;9 （<strong>嫁的个数为6+特征个数为3</strong>)<br>$$<br>\frac{1}{此特征数量+此标签的数量}<br>$$</p>
<h2><span id="损失函数loss-function或代价函数cost-function">损失函数(loss function)或代价函数(cost function)</span></h2><p>用于度量预测错误的程度。</p>
<h3><span id="平方损失函数quadratic-loss-function">平方损失函数(quadratic loss function)</span></h3><p>$$<br>L(Y,f(x)) &#x3D; (Y-f(x))^2<br>$$</p>
<h3><span id="经验损失empirical-loss-r_emp">经验损失(empirical loss) $R_{emp}$</span></h3><p>$$<br>R_{emp}(f)&#x3D;\frac{1}{N}\sum_i L(y_i,f(x_i))<br>$$</p>
<p>当样本容量很小时候，经验风险会产生过拟合。所以需要结构风险最小化(structural risk minimization,SRM),萎了防止过拟合:<br>$$<br>R_{srm}(f)&#x3D;\frac{1}{N}\sum_i L(y_i.f(x_i))+\lambda J(f)<br>$$</p>
<blockquote>
<p>J(f)为模型复杂度，$\lambda \ge 0$</p>
</blockquote>
<p>结构风险小，需要经验风险与复杂度同时小<br>$$<br>min_f\frac{1}{N}\sum_i L(y_i.f(x_i))+\lambda J(f)<br>$$</p>
<h3><span id="regression-loss">Regression Loss</span></h3><h4><span id="l_1-loss-x2f-laplace-loss-x2f-absolute-loss">$l_1$ Loss &#x2F; Laplace Loss &#x2F; Absolute Loss</span></h4><p>$l_1$ &#x3D; $\lvert r \rvert$</p>
<blockquote>
<p>缺点：不可微分</p>
</blockquote>
<h4><span id="l_2-loss-x2f-square-loss">$l_2$ Loss &#x2F; Square Loss</span></h4><p>$l_2$ &#x3D; $r^2$</p>
<blockquote>
<p>缺点：$l_2$ 受outlier 的影响比$l_1$影响大，所以不robust</p>
</blockquote>
<h4><span id="huber-loss">Huber Loss</span></h4><blockquote>
<p>robust 而且可微分 </p>
</blockquote>
<p><img src="https://upload-images.jianshu.io/upload_images/27695029-4f327793a9d758b6.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Loss 对比.JPG"></p>
<h3><span id="classification-loss">Classification Loss</span></h3><h4><span id="margin">Margin</span></h4><p>$$1\ if\ y &#x3D; \hat{y}$$<br>$$-1\ if\ y\neq\hat{y}$$</p>
<blockquote>
<p>不可微，非凸</p>
</blockquote>
<h4><span id="zero-one">Zero-one</span></h4><p>$l_{0-1}&#x3D;1\ if\ m\leq0$ </p>
<blockquote>
<p>m&gt;0 分类正确<br>##SVM&#x2F;Hinge<br>$l_{Hinge}&#x3D;max(1-m,0)$<br>##Logistic&#x2F;Log loss<br>$l_{logistic} &#x3D; log(1+e^{-m})$<br>可微</p>
</blockquote>
<blockquote>
<p>对比</p>
<p><img src="https://upload-images.jianshu.io/upload_images/27695029-6c25b23439b92b7a.JPG?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="z_h_l.JPG"></p>
</blockquote>
<h2><span id="gramm矩阵">Gramm矩阵</span></h2><p>如果有x1,x2,x3,则:<br>$$<br>\left[<br> \begin{matrix}<br>   x_1\cdot x_1 &amp; x_2\cdot x_1 &amp; x_3\cdot x_1 \<br>   x_1\cdot x_2 &amp; x_2\cdot x_2 &amp; x_3\cdot x_2 \<br>   x_1\cdot x_3 &amp; x_2\cdot x_3 &amp; x_3\cdot x_3<br>  \end{matrix}<br>  \right] \tag{3}<br>$$</p>
<p>主要用于Kernel计算</p>
<h2><span id="mercers-theorem">Mercer’s Theorem</span></h2><p>主要用于判别，是否为Kernel function</p>
<p>半正定矩阵(Positive Semidefinite, psd)：<br>$$<br>X^TMX \geq 0<br>$$<br>with M &#x3D; $R^TR$, and Eigenvalue of M $\geq$ 0</p>
<h2><span id="kernel-trick">Kernel Trick</span></h2><p>用于线性不可分的情况下，提升维度<br>$$<br>&lt;\Phi(x),\Phi(x^{‘})&gt; &#x3D; &lt;x,x^{‘}&gt;^2<br>$$</p>
<blockquote>
<p>即核函数的内积等于原来的内积的平方</p>
</blockquote>
<p>线性核函数：<br>$$<br>\Phi(x) &#x3D; (x^2,\sqrt2xy,y^2)<br>\<br>x&#x3D;(x_1,y_1),x^{‘} &#x3D; (x_2,y_2)<br>\<br>&lt;x,x^{‘}&gt;^2&#x3D;(x_1x_2+y_1y_2)^2&#x3D;x_1^2x_2^2+2x_1x_2y_1y_2+y_1y_2^2&#x3D;&lt;\Phi(x),\Phi(x^{‘})&gt;<br>$$</p>
<h2><span id="杰森不等式jensens-inequality">杰森不等式(Jensen’s inequality)</span></h2><p>杰森不等式其实就是凸函数的定义<br>$$<br>tf(x_1)+(1-t)f(x_2)\geq f(tx_1+(1-t)x_2)<br>$$</p>
<p>$$<br>E(f(x)) \geq f(E(x))<br>$$</p>
<h2><span id="凸函数">凸函数</span></h2><p>凸函数的充要条件是Hessian矩阵为半正定，且jacobian矩阵为0，即一阶导数为0。 一阶导数为0说明其为一个stationary point,二阶导数为零，说明斜率是单调递增的。<br>$$<br>f^{‘’}(x) \geq 0<br>$$</p>
<p><img src="https://img-blog.csdnimg.cn/20181102194631741.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTA0MjAyODM=,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>，</p>
<h2><span id="weak-max-min-inequality">Weak Max-Min Inequality</span></h2><p>$$<br>\inf f(w,z_0) \leq f(w_0,z_0) \leq supf(w_0,z)<br>$$</p>
<p>$$<br>d^*&#x3D;sup\ inf\ f(w,z) \leq inf\ sup\ f(w,z)&#x3D;p^*<br>$$</p>
<h2><span id="松弛互补complementary-slackness">松弛互补（Complementary slackness）</span></h2><p>$$<br>p^*&#x3D;d^*&#x3D;f_0(x^*)&#x3D;g(\lambda^*)&#x3D;L(x^*,\lambda^*) \Rightarrow \lambda_i^<em>f_i(x^</em>)&#x3D;0<br>$$</p>
<h2><span id="两平行线距离公式">两平行线距离公式</span></h2><p>$$<br>Ax+By&#x3D;C_1\ and\ Ax+By&#x3D;C_2<br>$$</p>
<p>$$<br>Distance &#x3D; \frac{\mid C_1-C_2\mid}{\sqrt{A^2+B^2}}<br>$$</p>
<h2><span id="pdfprobability-density-function">PDF(Probability density function)</span></h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/b/ba/Normalverteilung.PNG" alt="z_h_l.JPG"></p>
<h2><span id="cdfcumulative-distribution-function">CDF(Cumulative distribution function)</span></h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/Normal_Distribution_CDF.svg/1920px-Normal_Distribution_CDF.svg.png" alt="z_h_l.JPG"></p>
<h2><span id="pmfprobability-mass-function">PMF(probability mass function)</span></h2><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/85/Discrete_probability_distrib.svg/1920px-Discrete_probability_distrib.svg.png" alt="z_h_l.JPG"></p>
<h2><span id="泊松分布poisson-distribution">泊松分布(Poisson distribution)</span></h2><p>是离散分布<br>$$<br>P(X&#x3D;k) &#x3D; \frac{e^{-\lambda}\lambda^k}{k!}<br>$$<br>PMF:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/16/Poisson_pmf.svg/488px-Poisson_pmf.svg.png" alt="z_h_l.JPG"></p>
<p>CDF:</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Poisson_cdf.svg/1024px-Poisson_cdf.svg.png" alt="z_h_l.JPG"></p>
<h2><span id="variance">Variance</span></h2><p>$$<br>Var(X) &#x3D; {E(X-\mu)^2}\<br>&#x3D;E(X^2)-2E(X)E(X) + E(X)^2\<br>&#x3D;E(X^2)-E(X)^2\<br>E(X^2)&#x3D;\sigma^2+\mu^2<br>$$</p>
<h2><span id="伯努利分布和二项分布">伯努利分布和二项分布</span></h2><p>二项分布就是很多次伯努利分布</p>
<h2><span id="z分布和t分布">Z分布和T分布</span></h2><p>T分布用于小样本，Z分布用于大样本，当n大于等于30时，Z分布和T分布接近</p>
<p>Z分布：<br>$$<br>z &#x3D; \frac{x-\mu}{\sigma&#x2F;\sqrt{n}}<br>$$<br>T分布：<br>$$<br>t&#x3D;\frac{x-\mu}{s&#x2F;\sqrt{n}}\ with\ s样本方差：\frac{\sum(x-\mu)}{n-1}<br>$$</p>
<h2><span id="蒙特卡洛积分">蒙特卡洛积分</span></h2><p>$$<br>F&#x3D;\frac{1}{N}\sum{\frac{f(x)}{pdf(x)}}<br>$$</p>
<p><img src="https://www.qiujiawei.com/images/2016.8/1.png" alt="z_h_l.JPG"></p>
<h2><span id="判别模型和生成模型">判别模型和生成模型</span></h2><p>二者目的都是在使后验概率最大化，判别式是直接对后验概率建模，但是生成模型通过贝叶斯定理这一“桥梁”使问题转化为求联合概率</p>
<p>假设有四个samples： </p>
<p><img src="https://pic1.zhimg.com/50/v2-5eb5e035330831cd01d8fbea8d5b51e7_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic1.zhimg.com/80/v2-5eb5e035330831cd01d8fbea8d5b51e7_1440w.jpg?source=1940ef5c" alt="img"></p>
<p>生成式模型的世界是这个样子：</p>
<p><img src="https://pica.zhimg.com/50/v2-b082814755a62fcf676f3d08c70d2d0d_720w.jpg?source=1940ef5c" alt="img"><img src="https://pica.zhimg.com/80/v2-b082814755a62fcf676f3d08c70d2d0d_1440w.jpg?source=1940ef5c" alt="img"><br><img src="https://www.zhihu.com/equation?tex=%5CSigma+P(x,+y)+=+1" alt="[公式]"></p>
<p>而判定式模型的世界是这个样子：</p>
<p><img src="https://pic2.zhimg.com/50/v2-11f8a39d629e2649146bf12794fe310c_720w.jpg?source=1940ef5c" alt="img"><img src="https://pic2.zhimg.com/80/v2-11f8a39d629e2649146bf12794fe310c_1440w.jpg?source=1940ef5c" alt="img"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Csum_%7By%7D%7BP(y+%7C+x)%7D+=+1+" alt="[公式]"></p>
<h2><span id="极大似然估计mle">极大似然估计(MLE)</span></h2><p>知道数据来推求模型参数，假设不知道$\theta$ 的先验分布<br>$$<br>P(X|\theta)&#x3D;\prod\theta^x(1-\theta)^{1-x}<br>$$</p>
<p>$$<br>log(y) &#x3D; \sum xlog\theta+(1-x)(1-\theta)<br>$$</p>
<p>$$<br>d(logy)&#x3D;\frac{\sum x}{\theta}+\frac{\sum 1-x}{1-\theta}&#x3D;0 \<br>\theta &#x3D; \frac{\sum x}{n}<br>$$</p>
<h2><span id="最大后验概率-map">最大后验概率 (MAP)</span></h2><p>假设$\theta$ 符合一定的先验分布，则后验概率为<br>$$<br>P(\theta|X)&#x3D;\frac{P(X|\theta)P(\theta)}{P(X)}<br>$$<br>P(X)为常数，可省略<br>$$<br>log(y)&#x3D;log(P(X|\theta))+log(P(\theta))<br>$$</p>
<p>$$<br>d(logy)&#x3D;\frac{\sum x}{n} + log(P(\theta))&#x3D;0<br>$$</p>
<h2><span id="泰勒展开">泰勒展开</span></h2><p>$$<br>g(x)\approx g(x_0)+ \frac{f^{‘}(x_0)}{1!}(x-x_0)+\frac{f^{‘’}(x_0)}{2!}(x-x_0)<br>$$</p>
<h2><span id="梯度下降法">梯度下降法</span></h2><p>$$<br>J(\theta) &#x3D; \frac{1}{2m}\sum_i(y-\sum_j\theta x)^2<br>$$</p>
<p>$$<br>\frac{\delta J(\theta)}{\delta \theta_j} &#x3D; -\frac{1}{m}\sum_i(y-\sum \theta x)x_j<br>$$</p>
<p>$$<br>\theta_{j+1} &#x3D; \theta_j - \frac{\delta J(\theta)}{\delta \theta_j}<br>$$</p>
<h2><span id="牛顿法">牛顿法</span></h2><p>基本牛顿法是一种是用导数的算法，它每一步的迭代方向都是沿着当前点函数值下降的方向。<br>    我们主要集中讨论在一维的情形，对于一个需要求解的优化函数，求函数的极值的问题可以转化为求导函数。对函数进行泰勒展开到二阶，得到</p>
<p><img src="http://latex.codecogs.com/gif.latex?f%5Cleft&space;(&space;x&space;%5Cright&space;)=f%5Cleft&space;(&space;x_k&space;%5Cright&space;)+%7Bf%7D%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%5Cleft&space;(&space;x-x_k&space;%5Cright&space;)+%5Cfrac%7B1%7D%7B2%7D%7Bf%7D%27%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%5Cleft&space;(&space;x-x_k&space;%5Cright&space;)%5E2" alt="z_h_l.JPG"></p>
<p>对上式求导并令其为0，则为</p>
<p><img src="http://latex.codecogs.com/gif.latex?%7Bf%7D%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)+%7Bf%7D%27%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%5Cleft&space;(&space;x-x_k&space;%5Cright&space;)=0" alt="z_h_l.JPG"></p>
<p>即得到</p>
<p><img src="http://latex.codecogs.com/gif.latex?x=x_k-%5Cfrac%7B%7Bf%7D%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%7D%7B%7Bf%7D%27%27%5Cleft&space;(&space;x_k&space;%5Cright&space;)%7D" alt="z_h_l.JPG"></p>
<p>这就是牛顿法的更新公式。</p>
<h1><span id="机器学习基础">机器学习基础</span></h1><h2><span id="无监督学习算法unsupervised-learning-algorithm">无监督学习算法（unsupervised learning algorithm）</span></h2><p>训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。</p>
<h2><span id="监督学习算法supervised-learning-algorithm">监督学习算法（supervised learning algorithm)</span></h2><p>训练含有很多特征的数据集，不过数据集中的样本都有一个标签（label）或目标（target）。</p>
<h2><span id="强化学习rein-forcement-learning算法">强化学习（rein-forcement learning）算法</span></h2><p>会和环境进行交互，所以学习系统和它的训练过程会有反馈回路</p>
<h2><span id="设计矩阵design-matrix">设计矩阵（design matrix）</span></h2><p>设计矩阵的每一行包含一个不同的样本。每一列对应不同的特征。</p>
<h2><span id="训练误差training-error">训练误差（training error）</span></h2><p>以线性回归为例子:<br>$$<br>\frac{1}{m_{train}}\lVert X_{train}w-y_{train}\rVert_2^2<br>$$</p>
<h2><span id="测试误差test-error">测试误差（test error）</span></h2><p>以线性回归为例子:</p>
<p>$$<br>\frac{1}{m_{test}}\lVert X_{test}w-y_{test}\rVert_2^2<br>$$</p>
<h2><span id="奥卡姆剃刀-occams-razor">奥卡姆剃刀 Occam’s Razor</span></h2><ul>
<li>If two models correctly predict the data, the one that makes fewer assumptions should be preferred because simplicity is desirable in itself.</li>
<li>If two models correctly predict the data, the one that makes fewer assumptions should be preferred because it is likely to have lower generalization error</li>
</ul>
<h2><span id="vapnik-chervonenkis-维度">Vapnik-Chervonenkis 维度</span></h2><p>VC维定义为该分类器能够分类的训练样本的最大数目。假设存在m 个不同x 点的训练集，分类器可以任意地标记该m 个不同的x 点，VC维被定义为m的最大可能值。</p>
<p>统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降</p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/model.JPG"></p>
<h2><span id="贝叶斯误差bayes-error">贝叶斯误差（Bayes error）</span></h2><p>从预先知道的真实分布p(x; y) 预测而出现的误差被称为贝叶斯误差</p>
<h2><span id="没有免费午餐定理no-freelunch-theorem">没有免费午餐定理（no freelunch theorem）</span></h2><p>在某种意义上，没有一个机器学习算法总是比其他的要好。</p>
<h2><span id="权重衰减weight-decay">权重衰减（weight decay）</span></h2><p>修改线性回归的训练标准。<br>$$<br>J(w) &#x3D; MSE + \lambda w^tw<br>$$<br>当$\lambda$非常大时，我们可以强迫模型学习到了一个没有斜率的函数。由于它只能表示一个常数函数，所以会导致欠拟合。取一个适当的$\lambda$时，学习算法能够用一个正常的形状来恢复曲率。即使模型能够用更复杂的形状来来表示函数，权重衰减鼓励用一个带有更小参数的更简单的模型来描述它。当权重衰减趋近于0会导致严重的过拟合.</p>
<h2><span id="正则化regularization">正则化（regularization）</span></h2><p>正则化是指我们修改学习算法，使其降低泛化误差而非训练误差。</p>
<p>正则化与模型的复杂度相关，模型越复杂，正则化值就越大。经验损失 + $\lambda J(\theta)$</p>
<h3><span id="lasso-regularization">Lasso Regularization</span></h3><p>$\lambda \parallel \omega \parallel_1$, where $\parallel \omega \parallel_1 $ &#x3D; $\mid \omega_1 \mid + \mid \omega_2 \mid + \dots+\mid \omega_d \mid$</p>
<h3><span id="ridge-regularization">Ridge Regularization</span></h3><p>$\lambda \parallel \omega \parallel_2^2$, where $\parallel \omega \parallel_1 $ &#x3D; $\omega_1^2 + \omega_2^2 + \dots+\omega_d^2$</p>
<h3><span id="为什么lasso-比ridge-更稀疏-ridge比lasso更平滑">为什么Lasso 比Ridge 更稀疏， Ridge比Lasso更平滑</span></h3><p><img src="https://pica.zhimg.com/v2-a026e24156e13a1d14c43df26b9bd2a4_r.jpg?source=1940ef5c"></p>
<p><img src="https://pica.zhimg.com/v2-f6edae58134c5a26687c3883af48d5d5_r.jpg?source=1940ef5c"></p>
<p><img src="https://pic3.zhimg.com/v2-3aaa69f70754c469bca5c8e4c3e161db_r.jpg?source=1940ef5c"></p>
<h2><span id="交叉验证">交叉验证</span></h2><p>将数据集分成固定的训练集和固定的测试集后，若测试集的误差很小，这将是有问题的。这些过程是基于在原始数据上随机采样或分离出的不同数据集上重复训练和测试的想法。最常见的是k-折交叉验证过程，将数据集分成k 个<br>不重合的子集。测试误差可以估计为k 次计算后的平均测试误差。在第i 次测试时，数据的第i 个子集用于测试集，其他的数据用于训练集。</p>
<h2><span id="预测误差measures-prediction-errormse">预测误差（measures prediction error,MSE）</span></h2><p>$$<br>MSE &#x3D; E[(\hat \theta-\theta)^2]&#x3D;Bias(\hat \theta)^2+Var(\hat \theta)&#x3D;E[\hat \theta-\theta]^2+E[(\hat \theta-E[\theta])^2]<br>$$</p>
<p>用MSE度量泛化误差（偏差和方差对于泛化误差都是有意义的）时，增加容量会增加方差，降低偏差，即Bias,Variance Tradoff. 当模型复杂时，bias往往时比较小的，这时候误差主要来源于Variance。反之，模型比较简单，Bias较大，Variance较小。Bias代表的是模型的预测准确性，Variance代表的是模型对特定数据集的敏感程度</p>
<p><img src="https://raw.githubusercontent.com/PPPPan/PPPPan.github.io/master/images/MSE.JPG"></p>
<h2><span id="underfitting-overfitting">Underfitting, Overfitting</span></h2><p>Underfitting: High bias.</p>
<ol>
<li>模型过于简单</li>
<li>特征和预测结果相关性太低，无法提供有价值的信息</li>
</ol>
<p>Overfitting: High variance</p>
<ol>
<li>模型过于复杂</li>
<li>特征太多，训练数据集太小</li>
</ol>
<p>解决overfitting:</p>
<ol>
<li>选择简单模型</li>
<li>降低数据维度</li>
<li>增加数据量</li>
<li>Regularization,其实也是在控制复杂度，因为L1和L2是与模型复杂度相关的</li>
</ol>
<h2><span id="感知机perceptron">感知机(Perceptron)</span></h2><p>为二项线性分类模型<br>$$<br>f(x)&#x3D;sign(wx+b)<br>$$</p>
<blockquote>
<p>+1 x$\ge$ 0</p>
<p>-1 x&lt;0</p>
</blockquote>
<p>任一点$x_0$到超平面S的距离:<br>$$<br>\frac{1}{\lVert w \rVert}|wx_0+b|<br>$$<br>对于错误分类的点:<br>$$<br>-\frac{1}{\lVert w \rVert}y_i(wx_0+b)<br>$$<br>所有错误点到超平面S的总距离:<br>$$<br>-\frac{1}{\lVert w \rVert}\sum_xy_i(wx_0+b)<br>$$<br>损失函数:<br>$$<br>L(w,b) &#x3D; -\sum_i y_i(wx_i+b)<br>$$</p>
<blockquote>
<p>$\lVert w \rVert$ &#x3D; 1</p>
</blockquote>
<p>即求损失函数的最小值:<br>$$<br>min_{w,b}L(w,b) &#x3D; -\sum_i y_i(wx_i+b)<br>$$<br>感知机流程:</p>
<ol>
<li><p>选取 w0&#x3D;0,b0&#x3D;0</p>
</li>
<li><p>选择第一个点带入，得到$sign(wx_i+b)(wx_i+b)$</p>
</li>
<li><p>if $sign(wx_i+b)(wx_i+b) \le 0$</p>
<ol>
<li>w &#x3D; w+y*x</li>
<li>b&#x3D;b+y</li>
</ol>
</li>
<li><p>转至2，知道没有错误分类点</p>
</li>
</ol>
<p><strong>感知机的对偶形式:</strong></p>
<p>将模型改写为:<br>$$<br>L(w,b) &#x3D; y_i(\sum_j \alpha_j y_j x_j x_i+b)<br>$$</p>
<h2><span id="监督学习">监督学习</span></h2><h3><span id="逻辑回归logistic-regression">逻辑回归（logistic regression）</span></h3><p>该模型用于分类而非回归。logistic sigmoid 函数， 分为两类。</p>
<h3><span id="支持向量机support-vector-machine-svm">支持向量机（support vector machine, SVM)</span></h3><p>基于线性函数$w^Tx+b$，当为正时，支持向量机预测属于正类。为负数时候，支持向量机预测属于负类。</p>
<p>支持向量机中的线性函数可以重写为:<br>$$<br>w^Tx+b&#x3D;b+\sum_i \alpha_i x^Tx^{(i)}<br>$$</p>
<blockquote>
<p>$x^{(i)}$是训练样本,$\alpha$是系数向量</p>
</blockquote>
<p>我们将x 替换为特征函数ϕ(x) 的输出，点积替换为被称为<strong>核函数（kernel function）</strong>的函数</p>
<p>k(x, $x^{(i)}$) &#x3D; ϕ(x) ϕ($x^{(i)}$)。<br>$$<br>f(x)&#x3D;b+\sum_i \alpha_i k(x,x^{(i)})<br>$$<br>核技巧十分强大有两个原因。首先，它使我们能够使用保证有效收敛的凸优化技术来学习非线性模型（关于x 的函数）。这是可能的，因为我们可以认为ϕ 是固定的，仅优化$\alpha$，即优化算法可以将决策函数视为不同空间中的线性函数。其二，核函数k 的实现方法通常有比直接构建ϕ(x) 再算点积高效很多。</p>
<h3><span id="决策树">决策树</span></h3><p>常用的决策树有ID3，C4.5和CART（Classification And Regression Tree），CART的分类效果一般优于其他决策树。</p>
<p><strong>ID3: 信息增益来进行决策树的划分属性选择来决定那个做父节点，那个节点需要分裂。对于一组数据，信息增益越大说明分类结果越好</strong></p>
<p><img src="https://img-blog.csdnimg.cn/img_convert/9c16e9482a860b7f354bcd8a4d5c418e.png" alt="avatar"></p>
<ol>
<li>计算总体D</li>
<li>计算信息熵, 即总体的减去特征的，如果信息熵比较大，则说明次特征更稳定</li>
<li>计算信息增益，找出最大值为下个节点</li>
</ol>
<p>缺点：ID3算法会去选择子类别多的特征，因为这样分裂出来的结果会更纯，熵会更小，这有偏于我们的初衷，我们要的纯不是想通过让它分类分的更细，产生过拟合</p>
<p><strong>c4.5对ID3进行了改进，因为ID3会越分越细，可能会造成overfitting的情况C4.5中，优化项要除以分割太细的代价，这个比值叫做信息增益率，显然分割太细分母增加，<u>信息增益率</u>会降低。除此之外，其他的原理和ID3相同。</strong></p>
<ol>
<li>计算类别信息熵，即总体的，不分属性的</li>
</ol>
<img src="https://img-blog.csdn.net/20160611221836609?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar" style="zoom:150%;">

<p>9个取样为正，5个为负</p>
<ol start="2">
<li><p>计算各个类别的信息熵，信息熵是条件熵，比如在天气为阴的时候的正负性。与熵不同的是，熵直接计算天气这一属性，而不去计算属性下的类别。</p>
<p><img src="https://img-blog.csdn.net/20160611221842349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"></p>
</li>
<li><p>计算信息增益，信息增益(Gain) &#x3D; 总体熵(H) - 条件熵(Info)。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，当然，选择该属性就可以更快更好地完成我们的分类目标</p>
<p><img src="https://img-blog.csdn.net/20160611221847422?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"></p>
</li>
<li><p>但是我们假设这样的情况，每个属性中每种类别都只有一个样本，那这样属性信息熵就等于零，根据信息增益就无法选择出有效分类特征。所以，C4.5选择使用信息增益率对ID3进行改进，这是ID3的局限性，即类别分的太细，产生overfitting</p>
</li>
<li><p>计算类别的熵</p>
<p><img src="https://img-blog.csdn.net/20160611221851021?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"></p>
</li>
<li><p>计算信息增益率，（下面写错了。。应该是IGR &#x3D; Gain &#x2F; H ）</p>
<img src="https://img-blog.csdn.net/20160611222004158?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar" style="zoom:200%;"></li>
</ol>
<p>$$<br>IGR &#x3D; \frac{H(D)-Info(类别)}{H(类别)}&#x3D;\frac{Gain}{H}<br>$$</p>
<ol start="7">
<li>信息增益率越高越好，减少因特征值多导致信息增益大的问题。</li>
</ol>
<p><img src="https://img-blog.csdn.net/20160611211339152?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="avatar"></p>
<p><strong>ID3和C4.5算法，只能处理分类不能处理回归。而CART（classification and regression tree）分类回归树算法，既可用于分类也可用于回归。</strong></p>
<p><strong>CART分类树算法使用基尼系数选择特征</strong>，基尼系数代表了模型的不纯度，<strong>基尼系数越小，不纯度越低，特征越好</strong>。</p>
<p>数据集D的纯度可用基尼值来度量：</p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7DGini(D)&=%5Csum_%7Bi=1%7D%5E%7Bn%7D%7Bp(x_i)*(1-p(x_i))%7D+%5C%5C&=1-%5Csum_%7Bi=1%7D%5E%7Bn%7D%7B%7Bp(x_i)%7D%5E2%7D%5Cend%7Baligned%7D++%5C%5C" alt="avatar"></p>
<h3><span id="knn">KNN</span></h3><p>输入带标签的数据训练，然后给新输入的数据与已知数据比对，找出前k个最近的数据，在这k个相近的数据中，给新输入的数据填上最接近的标签。</p>
<ol>
<li>假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。</li>
<li>输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。<ol>
<li>计算新数据与样本数据集中每条数据的距离。</li>
<li>对求得的所有距离进行排序（从小到大，越小表示越相似）。</li>
<li>取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。</li>
</ol>
</li>
<li>求 k 个数据中出现次数最多的分类标签作为新数据的分类。</li>
</ol>
<h2><span id="k-mean-算法">K Mean 算法</span></h2><ul>
<li>随机选K个值作为k个聚类的平均值</li>
<li>讲每个样本与这几个均值作距离，将其归为最近的类</li>
<li>得到新的三个聚类，再求均值</li>
<li>再重复，直至收敛(mean 不发生变化)</li>
</ul>
<h2><span id="em算法">EM算法</span></h2><p>用于概率模型中含有观测变量(observable variable),以及隐变量(latent variable).</p>
<p>主要分为两部分:</p>
<ul>
<li>E部分，求期望 Expectation</li>
<li>M部分，求极大Maximization</li>
</ul>
<p>所以称为期望极大算法。</p>
<p>假设我们有两枚硬币，A与B</p>
<p>但是我们在投掷的过程中不记录硬币的类型，而是硬币的正反面。那么我们如何求A与B各自的概率呢。</p>
<p>首先假设A的概率为0.6，B的概率为0.5</p>
<p>则A的概率为:<br>$$<br>P_A &#x3D; \frac{0.6^5 0.4^5}{(0.6^5 0.4^5)+(0.5^50.5^5)} &#x3D; 0.45<br>$$</p>
<h2><span id="bootstrap-采样法">bootstrap 采样法</span></h2><p>即有放回的采样方法。</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2fe7849758309b19fa49cad2f5e214fb4fbb8044" alt="avatar"></p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e76702c87ce1c681ed1da8213125963524ca0ee6" alt="avatar"></p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5decdfc7edb14bc3f4dd541d39559ea9a3088f61" alt="avatar"><br>$$<br>\frac{1}{e}&#x3D;lim(1-\frac{1}{n})^n<br>$$<br>则，从来没取到的概率为0.368，取到的概率为0.632</p>
<h2><span id="线性回归">线性回归</span></h2><p>优化最小误差<br>$$<br>\frac{1}{2}min\sum{(y_i-w^Tx_i)^2} &#x3D; \frac{1}{2}(y-Xw)^T(y-Xw)<br>$$<br>ordinary least squares or OLS solution:</p>
<p>D is small, N&lt;1000<br>$$<br>w_{OLS} &#x3D; (X^TX)^{-1}X^Ty<br>$$</p>
<h2><span id="随机森林">随机森林</span></h2><p>为bagging为思想的集成学习方法，利用很多弱分类器，然后经过一定的结合原理，而得到强分类器的算法。</p>
<p>弱分类器：</p>
<ul>
<li>具有较低的分类能力，但是比乱猜强</li>
<li>分类器之间存在差异性</li>
</ul>
<p>使用bootstrap采样则，从来没取到的概率为0.368，取到的概率为0.632。</p>
<p>在对预测输出进行结合时，Bagging通常<strong>对分类任务使用简单投票法</strong>，<strong>对回归任务使用简单平均法</strong></p>
<h2><span id="boosting-and-bagging">Boosting and bagging</span></h2><p>Boosting主要关注降低偏差，因此Boosting能基于泛化性能相当弱的学习器构建出很强的集成；Bagging主要关注降低方差，因此它在不剪枝的决策树、神经网络等学习器上效用更为明显。</p>
<h2><span id="xgboost">XGBoost</span></h2><p>对比原算法GBDT，XGBoost主要从下面三个方面做了优化：</p>
<ul>
<li><p>一是算法本身的优化：在算法的弱学习器模型选择上，对比GBDT只支持决策树，还可以直接很多其他的弱学习器。在算法的损失函数上，除了本身的损失，还加上了正则化部分。在算法的优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。算法本身的优化是我们后面讨论的重点。</p>
</li>
<li><p>二是算法运行效率的优化：对每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，方便前面说的并行选择。对分组的特征，选择合适的分组大小，使用CPU缓存进行读取加速。将各个分组保存到多个硬盘以提高IO速度。</p>
</li>
<li><p>三是算法健壮性的优化：对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。算法本身加入了L1和L2正则化项，可以防止过拟合，泛化能力更强。</p>
</li>
</ul>
<p>在GBDT损失函数L(y,ft−1(x)+ht(x))L(y,ft−1(x)+ht(x))的基础上，我们加入正则化项如下：<br>$$<br>Ω(ht)&#x3D;γJ+λ2∑_{j&#x3D;1}^Jw^2_{tj}<br>$$</p>
<h2><span id="local-linear-embedding-lle">Local Linear Embedding (LLE)</span></h2><ol>
<li>根据欧几里得距离，选择K个neighbor</li>
<li>$Cost &#x3D; (X-wX_{neighbor})(X-wX_{neighbor})^T, \sum w &#x3D; 1,如果不是邻居则w_j&#x3D;0$ -&gt; $Cost&#x3D;w^TCw,C&#x3D;(X-X_{neighbor})(X-X_{neighbor})$</li>
<li>用拉格朗日乘子法-&gt; 求出$2wC-\lambda&#x3D;0$  -&gt; $w等价于C^{-1}1$</li>
<li>然后映射回低维，同样保持相同的关系， $(Y-wY)^T(Y-wY)-&gt;Y^T(1-w)^T(1-w)Y$ , 我们设定，$Y^TY&#x3D;1, \sum Y &#x3D;0$ ,改写为，$Y^TMY$ </li>
<li>然后拉格朗日求梯度，$2YM&#x3D;2\lambda Y$, 所以映射的新坐标为M的Eigenvector，第一个特征向量为全一，所以要从第二个特征向量开始，即可完成降维</li>
</ol>
<h2><span id="sne-and-t-sne">SNE and t-SNE</span></h2><p>主要思想是高维与低维的分布一致或者类似，</p>
<h3><span id="sne">SNE</span></h3><p>SNE是高维度使用高斯分布来拟合，然后用KL散度来用已知的高维分布去得到低维分布，前后默认都是高斯分布。已知原来的高维分布，然后初始化低维数据，然后用梯度下讲拟合。</p>
<p>但是由于高斯分布对outlier敏感，所以SNE效果并不好，会出现重叠。</p>
<p>高维分布：</p>
<p><img src="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne_1.JPG?raw=true"></p>
<p>低维分布：</p>
<p><img src="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne2.JPG?raw=true"></p>
<p>使用KL散度去拟合，并用梯度最小化优化：</p>
<p><img src="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne3.JPG?raw=true"></p>
<p><img src="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne4.JPG?raw=true" alt="sne4.JPG"></p>
<p>算法流程：</p>
<p><img src="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne6.JPG?raw=true" alt="sne6.JPG"></p>
<h3><span id="t-sne">t-SNE</span></h3><p>t-SNE可以认为是对SNE的一个优化，基本思想一致，都是用低维的去拟合高维的，只是低维拟合不再使用高斯分布，而是使用t分布，因为t分布对outlier跟具有鲁棒性，所以效果更好。</p>
<p>低维分布使用t-分布：</p>
<p><img src="https://github.com/PPPPan/PPPPan.github.io/blob/master/images/sne5.JPG?raw=true" alt="sne5.JPG"></p>
<p>SNE主要用于高维数据的可视化。</p>
<h2><span id="cnn">CNN</span></h2><p>主要用于图片处理，因为图片通常维度很高，如果使用常规的网络，会使参数过多，使用convolution，用pattern提取特征，然后可以用pooling进一步降低维度。</p>
<p>pooling:</p>
<ol>
<li>max pooling或者average pooling</li>
<li>通常来说max pooling的效果更好</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/ML/" rel="tag"># ML</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/04/15/Plan/" rel="prev" title="Schedule(SS2021 TUB)">
      <i class="fa fa-chevron-left"></i> Schedule(SS2021 TUB)
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/04/16/git/" rel="next" title="git-command">
      git-command <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">向量和矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">向量的范数(norm)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">逆矩阵(matrix inversion)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">单位向量</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">正交矩阵（orthogonal matrix）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">特征分解（eigendecomposition)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text">奇异值分解（singular value decomposition, SVD)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">8.</span> <span class="nav-text">迹运算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">9.</span> <span class="nav-text">主成分分析（principal components analysis, PCA）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">10.</span> <span class="nav-text">概率论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.1.</span> <span class="nav-text">信任度(degree of belief):</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.2.</span> <span class="nav-text">归一化（normalized）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.3.</span> <span class="nav-text">离散均匀分布（uniform distribution）:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.4.</span> <span class="nav-text">概率密度函数（probability density function):</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">10.4.1.</span> <span class="nav-text">连续均匀分布:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.5.</span> <span class="nav-text">链式法则（chain rule）:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.6.</span> <span class="nav-text">独立性（independent）:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.7.</span> <span class="nav-text">期望（expectation）:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.8.</span> <span class="nav-text">方差（variance）:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.9.</span> <span class="nav-text">协方差（covariance）:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">10.10.</span> <span class="nav-text">常用概率分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">11.</span> <span class="nav-text">数值计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">11.1.</span> <span class="nav-text">Jacobian矩阵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">12.</span> <span class="nav-text">Hessian 矩阵</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">12.1.</span> <span class="nav-text">Karush–Kuhn–Tucker（KKT）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">13.</span> <span class="nav-text">距离选择公式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">13.1.</span> <span class="nav-text">欧几里得距离</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">13.2.</span> <span class="nav-text">曼哈顿距离</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">14.</span> <span class="nav-text">贝叶斯定理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">15.</span> <span class="nav-text">朴素贝叶斯(Naive Bayesian algorithm)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">15.1.</span> <span class="nav-text">拉普拉斯平滑处理(Laplace Smoothing)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">16.</span> <span class="nav-text">损失函数(loss function)或代价函数(cost function)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">16.1.</span> <span class="nav-text">平方损失函数(quadratic loss function)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">16.2.</span> <span class="nav-text">经验损失(empirical loss) $R_{emp}$</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">16.3.</span> <span class="nav-text">Regression Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">16.3.1.</span> <span class="nav-text">$l_1$ Loss &#x2F; Laplace Loss &#x2F; Absolute Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">16.3.2.</span> <span class="nav-text">$l_2$ Loss &#x2F; Square Loss</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">16.3.3.</span> <span class="nav-text">Huber Loss</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">16.4.</span> <span class="nav-text">Classification Loss</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">16.4.1.</span> <span class="nav-text">Margin</span></a></li><li class="nav-item nav-level-4"><a class="nav-link"><span class="nav-number">16.4.2.</span> <span class="nav-text">Zero-one</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">17.</span> <span class="nav-text">Gramm矩阵</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">18.</span> <span class="nav-text">Mercer’s Theorem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">19.</span> <span class="nav-text">Kernel Trick</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">20.</span> <span class="nav-text">杰森不等式(Jensen’s inequality)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">21.</span> <span class="nav-text">凸函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">22.</span> <span class="nav-text">Weak Max-Min Inequality</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">23.</span> <span class="nav-text">松弛互补（Complementary slackness）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">24.</span> <span class="nav-text">两平行线距离公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">25.</span> <span class="nav-text">PDF(Probability density function)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">26.</span> <span class="nav-text">CDF(Cumulative distribution function)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">27.</span> <span class="nav-text">PMF(probability mass function)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">28.</span> <span class="nav-text">泊松分布(Poisson distribution)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">29.</span> <span class="nav-text">Variance</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">30.</span> <span class="nav-text">伯努利分布和二项分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">31.</span> <span class="nav-text">Z分布和T分布</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">32.</span> <span class="nav-text">蒙特卡洛积分</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">33.</span> <span class="nav-text">判别模型和生成模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">34.</span> <span class="nav-text">极大似然估计(MLE)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">35.</span> <span class="nav-text">最大后验概率 (MAP)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">36.</span> <span class="nav-text">泰勒展开</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">37.</span> <span class="nav-text">梯度下降法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">38.</span> <span class="nav-text">牛顿法</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link"><span class="nav-number"></span> <span class="nav-text">机器学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.</span> <span class="nav-text">无监督学习算法（unsupervised learning algorithm）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">2.</span> <span class="nav-text">监督学习算法（supervised learning algorithm)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">3.</span> <span class="nav-text">强化学习（rein-forcement learning）算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">4.</span> <span class="nav-text">设计矩阵（design matrix）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">5.</span> <span class="nav-text">训练误差（training error）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">6.</span> <span class="nav-text">测试误差（test error）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">7.</span> <span class="nav-text">奥卡姆剃刀 Occam’s Razor</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">8.</span> <span class="nav-text">Vapnik-Chervonenkis 维度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">9.</span> <span class="nav-text">贝叶斯误差（Bayes error）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">10.</span> <span class="nav-text">没有免费午餐定理（no freelunch theorem）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">11.</span> <span class="nav-text">权重衰减（weight decay）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">12.</span> <span class="nav-text">正则化（regularization）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">12.1.</span> <span class="nav-text">Lasso Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">12.2.</span> <span class="nav-text">Ridge Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">12.3.</span> <span class="nav-text">为什么Lasso 比Ridge 更稀疏， Ridge比Lasso更平滑</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">13.</span> <span class="nav-text">交叉验证</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">14.</span> <span class="nav-text">预测误差（measures prediction error,MSE）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">15.</span> <span class="nav-text">Underfitting, Overfitting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">16.</span> <span class="nav-text">感知机(Perceptron)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">17.</span> <span class="nav-text">监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">17.1.</span> <span class="nav-text">逻辑回归（logistic regression）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">17.2.</span> <span class="nav-text">支持向量机（support vector machine, SVM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">17.3.</span> <span class="nav-text">决策树</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">17.4.</span> <span class="nav-text">KNN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">18.</span> <span class="nav-text">K Mean 算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">19.</span> <span class="nav-text">EM算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">20.</span> <span class="nav-text">bootstrap 采样法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">21.</span> <span class="nav-text">线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">22.</span> <span class="nav-text">随机森林</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">23.</span> <span class="nav-text">Boosting and bagging</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">24.</span> <span class="nav-text">XGBoost</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">25.</span> <span class="nav-text">Local Linear Embedding (LLE)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">26.</span> <span class="nav-text">SNE and t-SNE</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">26.1.</span> <span class="nav-text">SNE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link"><span class="nav-number">26.2.</span> <span class="nav-text">t-SNE</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">27.</span> <span class="nav-text">CNN</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Xinjian Pan  👨🏻‍💻</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:xinjianpanstudent@gmail.com" title="E-Mail → mailto:xinjianpanstudent@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Xinjian Pan  👨🏻‍💻</span>
</div>

<!--
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
